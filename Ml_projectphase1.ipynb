{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing the data"
      ],
      "metadata": {
        "id": "DDiHyR40P-4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First stage is data preprocessing, in which we will be removing the stop words , tokenizing the words, do the stemming the process as well as the handling the missing values"
      ],
      "metadata": {
        "id": "EDvkq9B9WonE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVbiwewVP5Gs",
        "outputId": "253733fb-e00f-4da3-8b05-20f822bb7887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to preprocess text data\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Join the preprocessed words back into a text\n",
        "    preprocessed_text = ' '.join(words)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(input_filename):\n",
        "    data = []\n",
        "    with open(input_filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            row_dict = {}\n",
        "            for i, val in enumerate(row):\n",
        "                row_dict[header[i]] = val\n",
        "            data.append(row_dict)\n",
        "    return data\n",
        "\n",
        "# Function to write data to a CSV file\n",
        "def write_data_to_csv(output_filename, data):\n",
        "    with open(output_filename, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Leader', 'preprocessed_text'])\n",
        "        for row in data:\n",
        "            writer.writerow([row['Leader'], row['preprocessed_text']])\n",
        "\n",
        "# Read data from the input CSV file\n",
        "input_data = read_data_from_csv('covid_statements.csv')\n",
        "\n",
        "# Initialize an empty list to store preprocessed data\n",
        "output_data = []\n",
        "\n",
        "# Loop through each row in the input data\n",
        "for row in input_data:\n",
        "    id = row['\\ufeffLeader']\n",
        "    preprocessed_text = preprocess_text(row['statement'])\n",
        "    found = False\n",
        "\n",
        "    # Check if the leader ID is already in the output_data list\n",
        "    for i in range(len(output_data)):\n",
        "        if id.lower() == output_data[i]['Leader'].lower():\n",
        "            # If found, append the preprocessed text to the existing data\n",
        "            output_data[i]['preprocessed_text'] += '\\n' + preprocessed_text\n",
        "            found = True\n",
        "            break\n",
        "\n",
        "    # If leader ID is not found, add a new entry to the output_data list\n",
        "    if not found:\n",
        "        output_data.append({'Leader': id, 'preprocessed_text': preprocessed_text})\n",
        "\n",
        "# Write the preprocessed data to a new CSV file\n",
        "write_data_to_csv('covid_statements_preprocessed.csv', output_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection Algorithms"
      ],
      "metadata": {
        "id": "BQC_zTCZRHhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information gain"
      ],
      "metadata": {
        "id": "K7o-YlZ8Qf06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectPercentile, mutual_info_classif\n",
        "\n",
        "# Function to process a single data row and extract features using mutual information\n",
        "def process_single_data(input_row):\n",
        "    # Extract the preprocessed text from the input row\n",
        "    preprocessed_text = input_row['preprocessed_text']\n",
        "\n",
        "    # Initialize CountVectorizer to convert the text into a bag-of-words representation\n",
        "    vectorizer = CountVectorizer()\n",
        "    features = vectorizer.fit_transform([preprocessed_text])\n",
        "\n",
        "    # Use mutual information for feature selection\n",
        "    selector = SelectPercentile(mutual_info_classif, percentile=50)\n",
        "    selected_features = selector.fit_transform(features, [preprocessed_text])\n",
        "\n",
        "    # Get the indices of the selected features\n",
        "    feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "    # Get the names of all features\n",
        "    selected_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Filter the selected feature names based on the indices\n",
        "    selected_feature_names = [selected_feature_names[idx] for idx in feature_indices]\n",
        "\n",
        "    return selected_feature_names\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(input_filename):\n",
        "    data = []\n",
        "    with open(input_filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            row_dict = {}\n",
        "            for i, val in enumerate(row):\n",
        "                row_dict[header[i]] = val\n",
        "            data.append(row_dict)\n",
        "    return data\n",
        "\n",
        "# Read data from the preprocessed CSV file\n",
        "input_data = read_data_from_csv('covid_statements_preprocessed.csv')\n",
        "\n",
        "# Process each row of data and extract features\n",
        "for i in range(len(input_data)):\n",
        "    selected_feature_names = process_single_data(input_data[i])\n",
        "\n",
        "# Write the selected features to a new CSV file\n",
        "with open('information_gain.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Leader', 'information_gain_features'])\n",
        "    for i in range(len(input_data)):\n",
        "        row = input_data[i]\n",
        "        selected_feature_names = process_single_data(row)\n",
        "        writer.writerow([row['Leader'], ' '.join(selected_feature_names)])\n"
      ],
      "metadata": {
        "id": "ckfkL1JdQWG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Squared Test"
      ],
      "metadata": {
        "id": "MJGsvWgzQkMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Function to process a single data row and extract features using chi-squared feature selection\n",
        "def process_single_data(input_row):\n",
        "    # Extract the preprocessed text from the input row\n",
        "    preprocessed_text = input_row['preprocessed_text']\n",
        "\n",
        "    # Initialize CountVectorizer to convert the text into a bag-of-words representation\n",
        "    vectorizer = CountVectorizer()\n",
        "    features = vectorizer.fit_transform([preprocessed_text])\n",
        "\n",
        "    # Use chi-squared feature selection with k=4 (select top 4 features)\n",
        "    selector = SelectKBest(chi2, k=4)\n",
        "    selected_features = selector.fit_transform(features, [preprocessed_text])\n",
        "\n",
        "    # Get the indices of the selected features\n",
        "    feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "    # Get the names of all features\n",
        "    selected_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Filter the selected feature names based on the indices\n",
        "    selected_feature_names = [selected_feature_names[idx] for idx in feature_indices]\n",
        "\n",
        "    return selected_feature_names\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(input_filename):\n",
        "    data = []\n",
        "    with open(input_filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            row_dict = {}\n",
        "            for i, val in enumerate(row):\n",
        "                row_dict[header[i]] = val\n",
        "            data.append(row_dict)\n",
        "    return data\n",
        "\n",
        "# Read data from the preprocessed CSV file\n",
        "input_data = read_data_from_csv('covid_statements_preprocessed.csv')\n",
        "\n",
        "# Process each row of data and extract features\n",
        "for i in range(len(input_data)):\n",
        "    selected_feature_names = process_single_data(input_data[i])\n",
        "\n",
        "# Write the selected features to a new CSV file\n",
        "with open('chi_squared.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Leader', 'chi_squared_features'])\n",
        "    for i in range(len(input_data)):\n",
        "        row = input_data[i]\n",
        "        selected_feature_names = process_single_data(row)\n",
        "        writer.writerow([row['Leader'], ' '.join(selected_feature_names)])\n"
      ],
      "metadata": {
        "id": "asZ6A9eaQiBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cfs"
      ],
      "metadata": {
        "id": "dAB5_G3bQxQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.feature_selection import SelectFpr\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import GenericUnivariateSelect\n",
        "\n",
        "# Function to process a single data row and extract features using mutual information feature selection\n",
        "def process_single_data(input_row):\n",
        "    # Extract the preprocessed text from the input row\n",
        "    preprocessed_text = input_row['preprocessed_text']\n",
        "\n",
        "    # Initialize CountVectorizer to convert the text into a bag-of-words representation\n",
        "    vectorizer = CountVectorizer(max_features=30)\n",
        "    features = vectorizer.fit_transform([preprocessed_text])\n",
        "\n",
        "    # Use mutual information feature selection with k='all' (select all features)\n",
        "    selector = SelectKBest(mutual_info_classif, k='all')\n",
        "    selected_features = selector.fit_transform(features, [preprocessed_text])\n",
        "\n",
        "    # Get the indices of the selected features\n",
        "    feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "    # Get the names of all features\n",
        "    selected_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Filter the selected feature names based on the indices\n",
        "    selected_feature_names = [selected_feature_names[idx] for idx in feature_indices]\n",
        "\n",
        "    return selected_feature_names\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(input_filename):\n",
        "    data = []\n",
        "    with open(input_filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            row_dict = {}\n",
        "            for i, val in enumerate(row):\n",
        "                row_dict[header[i]] = val\n",
        "            data.append(row_dict)\n",
        "    return data\n",
        "\n",
        "# Read data from the preprocessed CSV file\n",
        "input_data = read_data_from_csv('covid_statements_preprocessed.csv')\n",
        "\n",
        "# Process each row of data and extract features\n",
        "for i in range(len(input_data)):\n",
        "    selected_feature_names = process_single_data(input_data[i])\n",
        "\n",
        "# Write the selected features to a new CSV file\n",
        "with open('cfs.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Leader', 'cfs_features'])\n",
        "    for i in range(len(input_data)):\n",
        "        row = input_data[i]\n",
        "        selected_feature_names = process_single_data(row)\n",
        "        writer.writerow([row['Leader'], ' '.join(selected_feature_names)])\n"
      ],
      "metadata": {
        "id": "rox0B349Qons"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCC"
      ],
      "metadata": {
        "id": "RdDXGnJiQz-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Function to process a single data row and extract features using PCC (Pearson Correlation Coefficient) feature selection\n",
        "def process_single_data(input_row):\n",
        "    # Extract the preprocessed text from the input row\n",
        "    preprocessed_text = input_row['preprocessed_text']\n",
        "\n",
        "    # Initialize CountVectorizer to convert the text into a bag-of-words representation\n",
        "    vectorizer = CountVectorizer()\n",
        "    features = vectorizer.fit_transform([preprocessed_text])\n",
        "\n",
        "    # Use PCC feature selection with k=4 (select top 4 features)\n",
        "    selector = SelectKBest(f_classif, k=4)\n",
        "    selected_features = selector.fit_transform(features, [preprocessed_text])\n",
        "\n",
        "    # Get the indices of the selected features\n",
        "    feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "    # Get the names of all features\n",
        "    selected_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Filter the selected feature names based on the indices\n",
        "    selected_feature_names = [selected_feature_names[idx] for idx in feature_indices]\n",
        "\n",
        "    return selected_feature_names\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(input_filename):\n",
        "    data = []\n",
        "    with open(input_filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            row_dict = {}\n",
        "            for i, val in enumerate(row):\n",
        "                row_dict[header[i]] = val\n",
        "            data.append(row_dict)\n",
        "    return data\n",
        "\n",
        "# Read data from the preprocessed CSV file\n",
        "input_data = read_data_from_csv('covid_statements_preprocessed.csv')\n",
        "\n",
        "# Write the selected features to a new CSV file\n",
        "with open('pcc_features.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Leader', 'pcc_features'])\n",
        "    for i in range(len(input_data)):\n",
        "        row = input_data[i]\n",
        "        selected_feature_names = process_single_data(row)\n",
        "        writer.writerow([row['Leader'], ' '.join(selected_feature_names)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OghY1fgEQyNz",
        "outputId": "75b1aad2-c06c-442f-8535-c70526c65c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:109: RuntimeWarning: invalid value encountered in divide\n",
            "  msw = sswn / float(dfwn)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OCEAN SCORES\n"
      ],
      "metadata": {
        "id": "NIuKBjRgRDCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the Sentiment Intensity Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to compute OCEAN scores based on sentiment analysis\n",
        "def compute_ocean_scores(text):\n",
        "    # Get sentiment scores using VADER sentiment analysis\n",
        "    sentiment = sid.polarity_scores(text)\n",
        "\n",
        "    # Compute OCEAN scores based on sentiment scores\n",
        "    openness_score = round(((sentiment['pos'] + 1 - sentiment['neg']) / 2) * 10, 1)\n",
        "    conscientiousness_score = round(((sentiment['pos'] + sentiment['neg']) / 2) * 10, 1)\n",
        "    extraversion_score = round(((sentiment['pos'] + sentiment['neg'] + 1) / 2) * 10, 1)\n",
        "    agreeableness_score = round(((sentiment['pos'] + 1 - sentiment['neg']) / 2) * 10, 1)\n",
        "    neuroticism_score = round(((sentiment['neg'] + 1 - sentiment['pos']) / 2) * 10, 1)\n",
        "\n",
        "    # Return the computed OCEAN scores\n",
        "    return {\n",
        "        'openness': openness_score,\n",
        "        'conscientiousness': conscientiousness_score,\n",
        "        'extraversion': extraversion_score,\n",
        "        'agreeableness': agreeableness_score,\n",
        "        'neuroticism': neuroticism_score\n",
        "    }\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(mypersonality_final):\n",
        "    data = []\n",
        "    with open(mypersonality_final, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip the header row\n",
        "        for row in reader:\n",
        "            data.append(row[1])  # Assuming the text is in the second column (index 1)\n",
        "    return data\n",
        "\n",
        "# Function to write OCEAN scores to a new CSV file\n",
        "def write_output_to_csv(mypersonality_final, output_file):\n",
        "    with open(mypersonality_final, 'r') as input_csvfile, open(output_file, 'w', newline='') as output_csvfile:\n",
        "        reader = csv.reader(input_csvfile)\n",
        "        writer = csv.writer(output_csvfile)\n",
        "\n",
        "        # Write the header row with OCEAN score columns\n",
        "        writer.writerow(next(reader) + ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'])\n",
        "\n",
        "        # Process each row in the input CSV and write OCEAN scores to the output CSV\n",
        "        for row in reader:\n",
        "            text = row[1]  # Assuming the text is in the second column (index 1)\n",
        "            ocean_scores = compute_ocean_scores(text)\n",
        "            writer.writerow(row + [ocean_scores['openness'], ocean_scores['conscientiousness'], ocean_scores['extraversion'], ocean_scores['agreeableness'], ocean_scores['neuroticism']])\n",
        "\n",
        "# Read data from the CSV file\n",
        "dataset = read_data_from_csv('information_gain.csv')\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "print(len(dataset))\n",
        "\n",
        "# Loop through each row in the dataset and compute OCEAN scores (currently, the computed scores are not used or stored)\n",
        "for i in range(len(dataset)):\n",
        "    compute_ocean_scores(dataset[i])\n",
        "\n",
        "# Write OCEAN scores to a new CSV file\n",
        "write_output_to_csv('information_gain.csv', 'IG_OCEAN_Scores.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7ZRYehkRE7R",
        "outputId": "59459c49-8ce9-4571-995a-ac31d4da59b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the Sentiment Intensity Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to compute OCEAN scores based on sentiment analysis\n",
        "def compute_ocean_scores(text):\n",
        "    # Get sentiment scores using VADER sentiment analysis\n",
        "    sentiment = sid.polarity_scores(text)\n",
        "\n",
        "    # Compute OCEAN scores based on sentiment scores\n",
        "    openness_score = round(((sentiment['pos'] + 1 - sentiment['neg']) / 2) * 10, 1)\n",
        "    conscientiousness_score = round(((sentiment['pos'] + sentiment['neg']) / 2) * 10, 1)\n",
        "    extraversion_score = round(((sentiment['pos'] + sentiment['neg'] + 1) / 2) * 10, 1)\n",
        "    agreeableness_score = round(((sentiment['pos'] + 1 - sentiment['neg']) / 2) * 10, 1)\n",
        "    neuroticism_score = round(((sentiment['neg'] + 1 - sentiment['pos']) / 2) * 10, 1)\n",
        "\n",
        "    # Return the computed OCEAN scores\n",
        "    return {\n",
        "        'openness': openness_score,\n",
        "        'conscientiousness': conscientiousness_score,\n",
        "        'extraversion': extraversion_score,\n",
        "        'agreeableness': agreeableness_score,\n",
        "        'neuroticism': neuroticism_score\n",
        "    }\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(mypersonality_final):\n",
        "    data = []\n",
        "    with open(mypersonality_final, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip the header row\n",
        "        for row in reader:\n",
        "            data.append(row[1])  # Assuming the text is in the second column (index 1)\n",
        "    return data\n",
        "\n",
        "# Function to write OCEAN scores to a new CSV file\n",
        "def write_output_to_csv(mypersonality_final, output_file):\n",
        "    with open(mypersonality_final, 'r') as input_csvfile, open(output_file, 'w', newline='') as output_csvfile:\n",
        "        reader = csv.reader(input_csvfile)\n",
        "        writer = csv.writer(output_csvfile)\n",
        "\n",
        "        # Write the header row with OCEAN score columns\n",
        "        writer.writerow(next(reader) + ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'])\n",
        "\n",
        "        # Process each row in the input CSV and write OCEAN scores to the output CSV\n",
        "        for row in reader:\n",
        "            text = row[1]  # Assuming the text is in the second column (index 1)\n",
        "            ocean_scores = compute_ocean_scores(text)\n",
        "            writer.writerow(row + [ocean_scores['openness'], ocean_scores['conscientiousness'], ocean_scores['extraversion'], ocean_scores['agreeableness'], ocean_scores['neuroticism']])\n",
        "\n",
        "# Read data from the CSV file\n",
        "dataset = read_data_from_csv('chi_squared.csv')\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "print(len(dataset))\n",
        "\n",
        "# Loop through each row in the dataset and compute OCEAN scores (currently, the computed scores are not used or stored)\n",
        "for i in range(len(dataset)):\n",
        "    compute_ocean_scores(dataset[i])\n",
        "\n",
        "# Write OCEAN scores to a new CSV file\n",
        "write_output_to_csv('chi_squared.csv', 'CS_OCEAN_Scores.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHjPu5UIRFlA",
        "outputId": "34e0c465-99c1-47ae-f541-3173d4cad12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the Sentiment Intensity Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to compute OCEAN scores based on sentiment analysis\n",
        "def compute_ocean_scores(text):\n",
        "    # Get sentiment scores using VADER sentiment analysis\n",
        "    sentiment = sid.polarity_scores(text)\n",
        "\n",
        "    # Compute OCEAN scores based on sentiment scores\n",
        "    openness_score = round(((sentiment['pos'] + 1 - sentiment['neg']) / 2) * 10, 1)\n",
        "    conscientiousness_score = round(((sentiment['pos'] + sentiment['neg']) / 2) * 10, 1)\n",
        "    extraversion_score = round(((sentiment['pos'] + sentiment['neg'] + 1) / 2) * 10, 1)\n",
        "    agreeableness_score = round(((sentiment['pos'] + 1 - sentiment['neg']) / 2) * 10, 1)\n",
        "    neuroticism_score = round(((sentiment['neg'] + 1 - sentiment['pos']) / 2) * 10, 1)\n",
        "\n",
        "    # Return the computed OCEAN scores\n",
        "    return {\n",
        "        'openness': openness_score,\n",
        "        'conscientiousness': conscientiousness_score,\n",
        "        'extraversion': extraversion_score,\n",
        "        'agreeableness': agreeableness_score,\n",
        "        'neuroticism': neuroticism_score\n",
        "    }\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(mypersonality_final):\n",
        "    data = []\n",
        "    with open(mypersonality_final, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip the header row\n",
        "        for row in reader:\n",
        "            data.append(row[1])  # Assuming the text is in the second column (index 1)\n",
        "    return data\n",
        "\n",
        "# Function to write OCEAN scores to a new CSV file\n",
        "def write_output_to_csv(mypersonality_final, output_file):\n",
        "    with open(mypersonality_final, 'r') as input_csvfile, open(output_file, 'w', newline='') as output_csvfile:\n",
        "        reader = csv.reader(input_csvfile)\n",
        "        writer = csv.writer(output_csvfile)\n",
        "\n",
        "        # Write the header row with OCEAN score columns\n",
        "        writer.writerow(next(reader) + ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'])\n",
        "\n",
        "        # Process each row in the input CSV and write OCEAN scores to the output CSV\n",
        "        for row in reader:\n",
        "            text = row[1]  # Assuming the text is in the second column (index 1)\n",
        "            ocean_scores = compute_ocean_scores(text)\n",
        "            writer.writerow(row + [ocean_scores['openness'], ocean_scores['conscientiousness'], ocean_scores['extraversion'], ocean_scores['agreeableness'], ocean_scores['neuroticism']])\n",
        "\n",
        "# Read data from the CSV file\n",
        "dataset = read_data_from_csv('cfs.csv')\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "print(len(dataset))\n",
        "\n",
        "# Loop through each row in the dataset and compute OCEAN scores (currently, the computed scores are not used or stored)\n",
        "for i in range(len(dataset)):\n",
        "    compute_ocean_scores(dataset[i])\n",
        "\n",
        "# Write OCEAN scores to a new CSV file\n",
        "write_output_to_csv('cfs.csv', 'CFS_OCEAN_Scores.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfwMkEOBRL67",
        "outputId": "02e40acf-016a-4418-999c-f98ccad2110f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the Sentiment Intensity Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to compute OCEAN scores based on sentiment analysis\n",
        "def compute_ocean_scores(text):\n",
        "    # Get sentiment scores using VADER sentiment analysis\n",
        "    sentiment = sid.polarity_scores(text)\n",
        "\n",
        "    # Compute OCEAN scores based on sentiment scores\n",
        "    openness_score = round(((sentiment['pos'] + 1 - sentiment['neg']) / 2) * 10, 1)\n",
        "    conscientiousness_score = round(((sentiment['pos'] + sentiment['neg']) / 2) * 10, 1)\n",
        "    extraversion_score = round(((sentiment['pos'] + sentiment['neg'] + 1) / 2) * 10, 1)\n",
        "    agreeableness_score = round(((sentiment['pos'] + 1 - sentiment['neg']) / 2) * 10, 1)\n",
        "    neuroticism_score = round(((sentiment['neg'] + 1 - sentiment['pos']) / 2) * 10, 1)\n",
        "\n",
        "    # Return the computed OCEAN scores\n",
        "    return {\n",
        "        'openness': openness_score,\n",
        "        'conscientiousness': conscientiousness_score,\n",
        "        'extraversion': extraversion_score,\n",
        "        'agreeableness': agreeableness_score,\n",
        "        'neuroticism': neuroticism_score\n",
        "    }\n",
        "\n",
        "# Function to read data from a CSV file\n",
        "def read_data_from_csv(mypersonality_final):\n",
        "    data = []\n",
        "    with open(mypersonality_final, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip the header row\n",
        "        for row in reader:\n",
        "            data.append(row[1])  # Assuming the text is in the second column (index 1)\n",
        "    return data\n",
        "\n",
        "# Function to write OCEAN scores to a new CSV file\n",
        "def write_output_to_csv(mypersonality_final, output_file):\n",
        "    with open(mypersonality_final, 'r') as input_csvfile, open(output_file, 'w', newline='') as output_csvfile:\n",
        "        reader = csv.reader(input_csvfile)\n",
        "        writer = csv.writer(output_csvfile)\n",
        "\n",
        "        # Write the header row with OCEAN score columns\n",
        "        writer.writerow(next(reader) + ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'])\n",
        "\n",
        "        # Process each row in the input CSV and write OCEAN scores to the output CSV\n",
        "        for row in reader:\n",
        "            text = row[1]  # Assuming the text is in the second column (index 1)\n",
        "            ocean_scores = compute_ocean_scores(text)\n",
        "            writer.writerow(row + [ocean_scores['openness'], ocean_scores['conscientiousness'], ocean_scores['extraversion'], ocean_scores['agreeableness'], ocean_scores['neuroticism']])\n",
        "\n",
        "# Read data from the CSV file\n",
        "dataset = read_data_from_csv('pcc_features.csv')\n",
        "\n",
        "# Print the number of rows in the dataset\n",
        "print(len(dataset))\n",
        "\n",
        "# Loop through each row in the dataset and compute OCEAN scores (currently, the computed scores are not used or stored)\n",
        "for i in range(len(dataset)):\n",
        "    compute_ocean_scores(dataset[i])\n",
        "\n",
        "# Write OCEAN scores to a new CSV file\n",
        "write_output_to_csv('pcc_features.csv', 'PCC_OCEAN_Scores.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTbHnN7ZRPmX",
        "outputId": "d4b64964-c149-4870-99b9-a5152378504d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PERSONALITY prediction"
      ],
      "metadata": {
        "id": "fj9FumdTTUDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the step, we will be training the model with personality scores and then implement them with the features which are selected through the feature selection algorithsm"
      ],
      "metadata": {
        "id": "qQijtdqUWSH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the training data\n",
        "data = pd.read_csv('train.csv')\n",
        "\n",
        "# Load the data with OCEAN scores obtained from information gain feature selection\n",
        "daata = pd.read_csv('IG_OCEAN_Scores.csv')\n",
        "\n",
        "# Extract features (X) and target variable (y) from the training data\n",
        "X_train = data[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "y_train = data['personality']\n",
        "\n",
        "# Extract features (X_test) from the data with OCEAN scores\n",
        "y_test = daata[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "\n",
        "# Initialize a Support Vector Machine (SVM) classifier with a polynomial kernel\n",
        "clf = SVC(kernel='poly')\n",
        "\n",
        "# Train the SVM classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict personality labels for the data with OCEAN scores\n",
        "y_pred = clf.predict(y_test)\n",
        "\n",
        "# Add the predicted personality labels to the data with OCEAN scores\n",
        "daata['personality'] = y_pred\n",
        "\n",
        "# Save the updated data with predicted personality labels to a new CSV file\n",
        "daata.to_csv('IG_OCEAN_Scores.csv', index=False)\n"
      ],
      "metadata": {
        "id": "a89Un5TpRS-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the training data\n",
        "data = pd.read_csv('train.csv')\n",
        "\n",
        "# Load the data with OCEAN scores obtained from chi-squared feature selection\n",
        "daata = pd.read_csv('CS_OCEAN_Scores.csv')\n",
        "\n",
        "# Extract features (X) and target variable (y) from the training data\n",
        "X_train = data[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "y_train = data['personality']\n",
        "\n",
        "# Extract features (X_test) from the data with OCEAN scores\n",
        "y_test = daata[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "\n",
        "# Initialize a Support Vector Machine (SVM) classifier with a radial basis function (RBF) kernel\n",
        "clf = SVC(kernel='rbf')\n",
        "\n",
        "# Train the SVM classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict personality labels for the data with OCEAN scores\n",
        "y_pred = clf.predict(y_test)\n",
        "\n",
        "# Add the predicted personality labels to the data with OCEAN scores\n",
        "daata['personality'] = y_pred\n",
        "\n",
        "# Save the updated data with predicted personality labels to a new CSV file\n",
        "daata.to_csv('CS_OCEAN_Scores.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ZcISdFUHUz9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the training data\n",
        "data = pd.read_csv('train.csv')\n",
        "\n",
        "# Load the data with OCEAN scores obtained from CFS feature selection\n",
        "daata = pd.read_csv('CFS_OCEAN_Scores.csv')\n",
        "\n",
        "# Extract features (X) and target variable (y) from the training data\n",
        "X_train = data[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "y_train = data['personality']\n",
        "\n",
        "# Extract features (X_test) from the data with OCEAN scores\n",
        "y_test = daata[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "\n",
        "# Initialize a Support Vector Machine (SVM) classifier with a polynomial kernel\n",
        "clf = SVC(kernel='poly')\n",
        "\n",
        "# Train the SVM classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict personality labels for the data with OCEAN scores\n",
        "y_pred = clf.predict(y_test)\n",
        "\n",
        "# Add the predicted personality labels to the data with OCEAN scores\n",
        "daata['personality'] = y_pred\n",
        "\n",
        "# Save the updated data with predicted personality labels to a new CSV file\n",
        "daata.to_csv('CFS_OCEAN_Scores.csv', index=False)\n"
      ],
      "metadata": {
        "id": "3Wz-hw1VVamP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the training data\n",
        "data = pd.read_csv('train.csv')\n",
        "\n",
        "# Load the data with OCEAN scores obtained from PCC feature selection\n",
        "daata = pd.read_csv('PCC_OCEAN_Scores.csv')\n",
        "\n",
        "# Extract features (X) and target variable (y) from the training data\n",
        "X_train = data[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "y_train = data['personality']\n",
        "\n",
        "# Extract features (X_test) from the data with OCEAN scores\n",
        "y_test = daata[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "\n",
        "# Initialize a Support Vector Machine (SVM) classifier with a polynomial kernel\n",
        "clf = SVC(kernel='poly')\n",
        "\n",
        "# Train the SVM classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict personality labels for the data with OCEAN scores\n",
        "y_pred = clf.predict(y_test)\n",
        "\n",
        "# Add the predicted personality labels to the data with OCEAN scores\n",
        "daata['personality'] = y_pred\n",
        "\n",
        "# Save the updated data with predicted personality labels to a new CSV file\n",
        "daata.to_csv('PCC_OCEAN_Scores.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ykqB1qo6VkE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifiers"
      ],
      "metadata": {
        "id": "QifEfOlwjebP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this stage, we will be implementing the machine learning classifiers on the feature selection algorithms"
      ],
      "metadata": {
        "id": "OJOsMyzpWGTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pcc-svm"
      ],
      "metadata": {
        "id": "Yw6XcJuYjgsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the data with OCEAN scores obtained from PCC feature selection\n",
        "data = pd.read_csv('PCC_OCEAN_Scores.csv')\n",
        "\n",
        "# Extract features (X) and target variable (y) from the data\n",
        "X = data[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "y = data['personality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize a Support Vector Machine (SVM) classifier with a radial basis function (RBF) kernel\n",
        "clf = SVC(kernel='rbf')\n",
        "\n",
        "# Train the SVM classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict personality labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy_percentage = accuracy * 100\n",
        "\n",
        "# Print the accuracy of the model\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_percentage))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgOr0uRhjr8r",
        "outputId": "b3e47a93-86fc-4c13-98e4-79d6dd45f2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 83.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHI-SQUARED TEST --Decision Tree\n"
      ],
      "metadata": {
        "id": "6DZ9My2ljz2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Load the dataset\n",
        "data = pd.read_csv('CS_OCEAN_Scores.csv')\n",
        "X = data[['openness','conscientiousness','extraversion','agreeableness','neuroticism']]\n",
        "y = data['personality']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.45,random_state=42)\n",
        "\n",
        "# Train the decision tree classifier\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "# Test the classifier on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print the accuracy of the classifier\n",
        "accuracy1 = accuracy_score(y_test, y_pred)\n",
        "accuracy1 = accuracy1 * 100\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qoLsGAzjt7r",
        "outputId": "3607dccc-6418-4b92-ffb1-c5d0c698f8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CFS---Naive Bayes"
      ],
      "metadata": {
        "id": "MrPAPT6TlMFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data with OCEAN scores obtained from CFS feature selection\n",
        "data = pd.read_csv('CFS_OCEAN_Scores.csv')\n",
        "\n",
        "# Extract features (X) and target variable (y) from the data\n",
        "X = data[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "y = data['personality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
        "\n",
        "# Initialize a Multinomial Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Train the Naive Bayes classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict personality labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy_percentage = accuracy * 100\n",
        "\n",
        "# Print the accuracy of the model\n",
        "print(\"Naive Bayes Accuracy (CFS): {:.2f}%\".format(accuracy_percentage))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQsnqyrJlZJt",
        "outputId": "c88054c5-f787-4f60-a5b8-59ee078646ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Basis Accuracy CFS: 72.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IG---Random Forest"
      ],
      "metadata": {
        "id": "36Hap86dl4vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data with OCEAN scores obtained from Information Gain feature selection\n",
        "data = pd.read_csv('IG_OCEAN_Scores.csv')\n",
        "\n",
        "# Extract features (X) and target variable (y) from the data\n",
        "X = data[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']]\n",
        "y = data['personality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.57, random_state=42)\n",
        "\n",
        "# Initialize a Random Forest classifier with a fixed random state for reproducibility\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict personality labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy_percentage = accuracy * 100\n",
        "\n",
        "# Print the accuracy of the Random Forest model\n",
        "print(\"Random Forest Accuracy: {:.2f}%\".format(accuracy_percentage))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97lPhFQPmN0K",
        "outputId": "da8652fc-8f3b-4c4f-b57c-3718615ec885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 92.86%\n"
          ]
        }
      ]
    }
  ]
}